<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>将卷积神经网络与图卷积网络链接：在肺动脉-静脉分割中的应用</title>
    <url>/2020/07/14/paper-0/</url>
    <content><![CDATA[<p><img src="/2020/07/14/paper-0/image-1.png" alt="image-20220315194014722"></p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>图卷积网络（GCN）是处理非欧几里得数据的一种新颖而强大的方法，而卷积神经网络（CNN）可以从欧几里得数据中学习特征，例如图像。在这项工作中，本文提出了一种将CNN与GCN相结合的新方法（CNN-GCN），该方法可以同时考虑欧几里得特征和非欧几里得特征，并且可以端到端地进行训练。<span id="more"></span>用此方法将肺血管树分为动脉和静脉（A &#x2F; V）。通过血管分割和骨架化对胸部CT进行预处理，由此构成一个图：将骨架上的体素产生顶点集连接成邻接矩阵。从CT扫描中提取与每个血管垂直的3D像素块，训练CNN-GCN分类器，并将其应用于所构建的血管图上，然后将每个节点标记为动脉或静脉。该方法在一家医院(11名患者，22个肺)的数据上进行了训练和验证，并在另一家医院(10名患者，10个肺)的独立数据上进行了测试。使用基线CNN方法和人类观察者的表现进行比较，CNN-GCN方法获得的中位数准确性在验证（测试）集中为0.773（0.738），而观察者的中位数准确性为0.817，CNN的中位数准确性为0.727（0.693）。总之，本文所提出的CNN-GCN方法将局部图像信息与图形连接性信息相结合，与基线CNN方法相比改善了肺动静脉分割，接近了人类观察者的表现。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>图卷积网络（GCN）是卷积神经网络（CNN）的一种变体。近年来，有很多研究领域在处理图形数据（非欧几里得数据），例如社交网络，应用化学，计算机视觉等。GCN及其变体在这些方面获得了良好的性能。</p>
<p>GCN经过训练可以根据特征和连通性预测血管的存在，并使用推理模块生成最终的分割。该方法在视网膜血管和冠状动脉数据集上均取得了很好的结果。但是，该分类器无法进行端到端训练，这可能会产生次优的结果。</p>
<p>因此在这项工作中，本文提出了一种链接CNN和GCN的新型网络，该网络考虑了局部图像和图的连通性特征，并且可以端到端地训练分类器。为了使具有包含大量3D像素块的节点的图适应GPU内存，本文提出了一种基于批处理而不是整个图的策略，用于CNN-GCN训练和验证。 CNN-GCN方法用于分割肺动静脉，通过血管图构建和3D像素块提取对图像进行预处理，然后由CNN-GCN分类器预测血管图中每个节点的类别，并根据这些节点的分类对动静脉重建。</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="链接CNN与GCN"><a href="#链接CNN与GCN" class="headerlink" title="链接CNN与GCN"></a>链接CNN与GCN</h2><p>GCN可以结合局部和连通性信息，这对于分析血管可能很有用。为了将CNN与GCN结合起来，本文让CNN来学习特征矩阵X，X是从CNN获知的特征图，进行类似于Eq的正常GCN步骤，然后将CNN-GCN分类器链接成函数链，通过反向传播的梯度法直接优化。使用共享的CNN函数处理每个节点的像素块。</p>
<p>算法1中给出了该方法的伪代码。</p>
<p><img src="/2020/07/14/paper-0/image-2.png" alt="image-20220315194512828"></p>
<p>直接训练CNN-GCN要求将整个图形加载到GPU内存中，但随着特征的学习，这在当前的GPU中是不可行的，本文提出一个抽样策略而不是使用整个图形,类似于批处理训练CNN。</p>
<p>给定图G，随机选择b个具有像素块的节点作为输入batch B，大小为b×S。由于使用图结构，因此还需要这些邻接的像素块。由大小为b×n×S的NB表示，其中n是邻接点数。 batch B和相邻batch NB都使用共享的CNN函数处理，该函数可以由多层组成，例如卷积层，最大池层，激活层等。在每次迭代中，都会重新选择B和NB。</p>
<p>算法2中介绍了在图上具有批处理策略的GCN层的伪代码。</p>
<h2 id="在肺动脉-静脉分割中的应用"><a href="#在肺动脉-静脉分割中的应用" class="headerlink" title="在肺动脉-静脉分割中的应用"></a>在肺动脉-静脉分割中的应用</h2><p>通过血管分割方法从胸部CT扫描中提取肺血管树，然后通过骨架化方法进行骨架化。</p>
<p><img src="/2020/07/14/paper-0/image-3.png" alt="image-20220315194705196"></p>
<p>图1.肺动脉-静脉分割的方法</p>
<p>将血管骨架上的所有体素添加为一组节点V，并基于它们的连接构造邻接矩阵A。在这项研究中，仅考虑了一个维度（直接）的邻接点，分别为左右肺构造图G。对于血管骨架上的每个体素，从CT图像中提取出垂直于血管方向且大小为S &#x3D; [32，32，5]的局部像素块pi。基于中心体素的标签，将像素块pi标记为动脉或静脉。按照批处理策略，输入batch B及其邻居NB使用共享函数分别处理特征向量X和NX。将特征向量X和NX插入到GCN层，表示并转换到新的维数。在激活层之后，将以二维形式预测输出。通过连接CNN和GCN，肺动静脉分割的结构如图1所示。基于中心体素的预测，重新构建肺动静脉，其中横截面上的每个体素都标记有相应中心体素的预测。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>本文收集了中山大学附属医院的东芝Aquilion ONE扫描的11例对比增强CT扫描（SunYs数据集），以及由莱顿大学医学中心的ToshibaAquilion 64扫描的10例对比增强CT扫描（LUMC数据集）。通过滤波器对所有CT扫描进行重新采样，以获得大小为0.625mm3的各向同性体素</p>
<p><img src="/2020/07/14/paper-0/image-4.png" alt="image-20220315194758400"></p>
<p>图2.通过链接CNN和GCN层，构建用于肺动脉-静脉分割的CNN-GCN模型。C：卷积层；MP：最大池层；DO：dropout层；FC：全连接层</p>
<p>肺血管被分割并提供给每个实例。对于SunYs数据集，中山大学附属医院的两名放射科医生将分割的肺血管标记为动脉或静脉，作为初始标签。LUMC的三位专家对初始标签进行了检查和更正。校正后的标签用作基本事实，其中初始标签用于评估观察者的表现。总共从11名患者（共1,041,463个像素块）的22个肺中获得了完整标签的肺动脉静脉，称为SunYs数据集。根据此数据集，训练使用了16个肺（722,013个像素块），验证使用了6个肺（319,450个像素块）。对于LUMC数据集，每例病例的右或左肺血管均由两名专家分别标记，总共准备了10个具有完整肺动静脉标签的肺（504,527个像素块）作为独立的测试集。</p>
<p>用CNN3D架构做比较。使用Glorot随机初始化CNN-GCN方法中的函数的权重。另外，本文从CNN3D迁移了学习到的权重，以初始化CNN-GCN方法，将其称为“ CNN-GCNt”。使用相同的数据对所有这三种方法进行了训练和验证，并且它们的超参数设置保持不变。</p>
<p>learning rate&#x3D; 1e-3，batchsize&#x3D; 128，epoch &#x3D; 100。作为基准，观察者的初始标签已根据实际情况进行了验证。准确性被用作比较所有方法和观察者性能的关键指标。</p>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p>把经过训练的分类模型用于预测肺A &#x2F; V的标签，并根据实际情况对结果进行评估。结果如图3所示。使用来自SunYs数据集的验证集，CNN3D，CNNGCN和CNN-GCNt方法分别获得0.727、0.764和0.778的中位数准确性。相比之下，观察者的中位数准确性为0.817。对于独立测试集（LUMC数据集），这三种方法的中位数准确性分别为0.693、0.723和0.738。</p>
<p>示例A &#x2F; V分割结果如图4所示，显示了验证集的好坏情况。</p>
<p><img src="/2020/07/14/paper-0/image-5.png" alt="image-20220315194920034"></p>
<p>图3.训练、验证和测试集中的自动方法和观察者的准确性</p>
<p><img src="/2020/07/14/paper-0/image-6.png" alt="image-20220315194952373"></p>
<p>图4.（a）和（b）的2D可视化的结果，其中好结果中CNN3D，CNN-GCN和CNN-GCNt的精度分别为0.759、0.800和0.807；差结果的准确性是分别为0.687、0.710和0.724</p>
<h1 id="讨论与结论"><a href="#讨论与结论" class="headerlink" title="讨论与结论"></a>讨论与结论</h1><p>通过将CNN与GCN链接起来，本文提出了一种新型的基于深度学习的方法，该方法可以进行端到端的训练。CNN-GCN方法可以同时考虑局部图像和连接信息。提出了一种针对图形的局部批处理策略，以使具有大量节点的图形可在GPU内存中训练。在肺动脉-静脉分割的应用中，CNNGCN方法可以提供一次分割，其效果比CNN方法更好，但与观察者相比，其结果略差。该研究存在一些局限性。在测试数据集中，本文没有独立验证观察者的标记，因此在测试过程中没有提供观察者的表现。将来，可以考虑把校正后的标签添加到测试集中。训练期间可能发生过拟合，可以通过添加正则优化器或更多训练样本来克服。尽管存在这些局限性，本文还是从独立的测试集中获得了令人鼓舞的结果，尤其是考虑到这些结果来自不同的患者群体、CT协议和CT扫描仪。总之，本文所提出的CNN-GCN方法经过端到端的训练，成功地将图像和图形中的信息结合在一起。</p>
]]></content>
      <categories>
        <category>论文学习</category>
      </categories>
      <tags>
        <tag>图像分割</tag>
        <tag>CNN</tag>
        <tag>GCN</tag>
      </tags>
  </entry>
  <entry>
    <title>神经架构搜索：综述</title>
    <url>/2020/09/26/paper-1/</url>
    <content><![CDATA[<p><img src="/2020/09/26/paper-1/image-1.png" alt="image-20220315195628071"></p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>近几年，深度学习在许多任务上取得显著进步，如图像识别、语音识别和机器翻译。一个重要方面是新颖的神经网络结构，目前使用的架构大多是由人类专家手动开发，这是一个耗时且容易出错的过程。因此，人们对自动神经结构搜索方法越来越感兴趣。文章对这一领域的现有工作进行了综述，并对搜索空间、搜索策略和性能评估方法三个维度进行分类概述。</p>
<p>关键词：神经结构搜索、AutoML、AutoDL、搜索空间设计、搜索策略、性能评估方法</p>
<span id="more"></span>

<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p><img src="/2020/09/26/paper-1/image-2.png" alt="img"></p>
<p>图1：神经架构搜索方法。搜索策略从预定义的搜索空间A中选择体系结构a。该体系结构被传递给性能估计策略，该策略将a的估计性能返回给搜索策略。</p>
<p><strong>搜索空间。</strong>搜索空间定义了可以表示哪些结构。结合有关任务的体系结构的先验知识，可以减小搜索空间的大虽然NAS已经取得了令人印象深刻的性能，但是到目前为止，它还没有深入了解为什么特定的体系结构能够很好地工作，以及独立运行中派生的体系结构有多相似。识别常见的母题，提供一个理解为什么这些母题是重要的高性能，并调查这些母题是否适用于不同的问题是可取的。</p>
<p>小并简化搜索。然而，这也引入了人类的偏见，可能会阻止人们找到超越人类现有知识的新的架构构件。</p>
<p><strong>搜索策略。</strong>搜索策略说明了如何探索搜索空间。一方面，它包含了经典的勘探与开发权衡，希望快速找到性能良好的架构；而另一方面，应避免过早收敛到次优架构的区域。</p>
<p><strong>性能评估方法。</strong> NAS的目标通常是找到对看不见的数据实现高预测性能的体系结构。性能评估指的是评估这种性能的过程：最简单的是对数据进行标准的体系结构训练和验证，但这计算成本很大，并且限制了可以探索的体系结构的数量。因此，最近许多研究集中在开发降低这些性能评估成本的方法上。</p>
<h1 id="搜索空间"><a href="#搜索空间" class="headerlink" title="搜索空间"></a>搜索空间</h1><p>相对简单的搜索空间是链结构神经网络的空间，如图2（左）。链结构神经网络架构可以写为n层的序列，其中第i层Li从i-1层接收输入，其输出用作第i+1层的输入。然后，搜索空间参数化为：①层数n ②每层执行的操作，例如池化、卷积等 ③超参数，例如，滤波器数量、内核大小和跨距等。</p>
<p><img src="/2020/09/26/paper-1/image-3.png" alt="img"></p>
<p>图2：不同架构空间。左：链结构空间的元素。右：一个更复杂的搜索空间的元素，具有附加层类型和多个分支和跳过连接。</p>
<p>最近NAS的研究采用了现代设计元素，这些元素来自于手工构建的架构，如允许构建复杂的多分支网络的跳跃连接，如图2（右）。</p>
<p>受到由重复元素构成的手工架构的启发，最近研究建议直接搜索这些元素，称为单元或块，而不是整体架构。优化两种不同类型的单元：保持维度不变的正常单元和降低空间维度的缩小单元。最后的架构是以预定义的方式堆叠这些单元来构建的，如图3。与上述搜索空间相比，该搜索空间具有三大优势：</p>
<ol>
<li><p>搜索空间的大小减少了，因为单元通常由比整个体系结构少得多的层组成。</p>
</li>
<li><p>通过简单地改变模型中使用的单元和过滤器的数量，从单元构建的体系结构可以更容易地传输或适应其他数据集。</p>
</li>
<li><p>通过重复构建块来创建体系结构已被证明是一种有用的设计原则，例如在RNNs中重复LSTM块或堆叠残差块。</p>
</li>
</ol>
<p><img src="/2020/09/26/paper-1/image-4.png" alt="img"></p>
<p>图3：单元搜索空间。左：两个不同的单元，例如正常单元（顶部）和缩小单元（底部）。右：通过顺序堆叠单元构建的架构。</p>
<p>但当使用基于单元的搜索空间时，出现了一种新的设计选择，即如何选择宏体系结构：应使用多少个单元，它们应如何连接以构建实际模型？原则上，通过简单地用单元替换层，可以任意组合单元。理想情况下，宏观架构和微观架构应该联合优化，而不是仅仅优化微观架构；否则，很容易在找到性能良好的单元后，不得不手动进行宏观架构工程。</p>
<p>搜索空间的选择在很大程度上决定了优化问题的难度：即使对于基于固定宏结构的单个单元的搜索空间，优化问题仍然是非连续的、相对高维的。</p>
<h1 id="搜索策略"><a href="#搜索策略" class="headerlink" title="搜索策略"></a>搜索策略</h1><p>许多不同的搜索策略可以用来探索神经结构的空间，包括随机搜索、贝叶斯优化、进化方法、强化学习（RL）和基于梯度的方法。</p>
<p>将NAS框架化为强化学习问题，神经结构的生成可以看作是智能体的行为，其行为空间与搜索空间相同。代理奖励是基于对训练后的体系结构在看不见的数据上的性能的估计。不同的RL方法在如何表示代理策略以及如何优化策略方面有所不同。</p>
<p>这些方法的另一种观点是作为顺序决策过程，其中策略按顺序对操作进行采样以生成体系结构，环境的“状态”包含迄今为止采样的操作的摘要，并且奖励仅在最终操作之后获得。然而，由于在这个连续过程中没有与环境发生交互，文章发现将架构采样过程解释为单个动作的连续生成更为直观；这将RL问题简化为无状态的多臂老虎机问题。</p>
<p>使用RL的另一种选择是使用进化算法优化神经结构的神经进化方法。许多神经进化方法都使用遗传算法来优化神经结构及其权重；然而，当将具有数百万权重的神经结构用于监督学习任务时，基于SGD的权重优化方法优于进化方法。因此，再次使用基于梯度的方法来优化权重，并仅使用进化算法来优化神经结构本身。进化算法进化一组模型，即一组网络；在每一个进化步骤中，至少有一个模型从种群中取样，作为父代，通过对其应用突变来生成后代。在NAS的上下文中，突变是局部操作，例如添加或删除一个层，改变一个层的超参数，添加跳跃连接，以及改变训练超参数。训练后代后，评估其适应度（例如，在验证集上的表现），并将其添加到群体中。</p>
<p>贝叶斯优化是超参数优化最常用的方法之一，但由于典型的BO工具箱基于高斯过程，关注低维连续优化问题，因此许多研究小组尚未将其应用于NAS。一些工作使用基于树的模型来有效地搜索高维条件空间，并在广泛的问题上实现最先进的性能，同时优化神经结构及其超参数。虽然缺乏全面的比较，但有初步证据表明，这些方法也可以优于进化算法。</p>
<h1 id="性能评估策略"><a href="#性能评估策略" class="headerlink" title="性能评估策略"></a>性能评估策略</h1><p>搜索策略旨在找到一种神经架构，该架构可以最大化某些性能指标，例如对看不见的数据的准确性。为了指导搜索过程，这些策略需要估计考虑的给定架构A的性能。最简单的方法是对训练数据进行训练，并根据验证数据评估其性能。然而，对每个架构进行从头开始评估的培训经常会为NAS带来数千天的GPU计算需求，这自然会催生出用于加速性能评估的方法。关于现有方法的概述，参见表1。</p>
<p>性能可以根据完全训练后实际性能的较低可信度来估计。这样的低可信度包括较短的训练时间，对数据子集的训练，在较低分辨率图像上的训练等等。尽管这些低可信度近似值降低了计算成本，但由于性能通常会被低估，因此它们还会在估计中引入偏差。只要搜索策略仅依赖于对不同体系结构进行排名并且相对排名保持稳定，这可能就不会出现问题。然而，最近的结果表明，当廉价近似值与“完整”评估值之间的差异太大时，这种相对排名可能会发生显著变化。</p>
<p><img src="/2020/09/26/paper-1/image-5.png" alt="img"></p>
<p>表1：Nas中加速性能评估的不同方法</p>
<p>评估架构性能的另一种可能方法是基于学习曲线外推，外推初始学习曲线，并终止预测性能较差的曲线，以加快架构搜索过程。预测神经体系结构性能的主要挑战是，为了加快搜索过程，需要在相对较小的评价基础上，在相对较大的搜索空间中进行良好的预测。</p>
<p>另一种加速性能评估的方法是根据之前训练过的其他体系结构的权重初始化新体系结构的权重。实现这一点的一种方法称为网络变形，允许在保持网络表示的功能不变的情况下修改架构，从而产生只需要几天GPU计算的方法。这允许连续地增加网络的容量并保持高性能，而不需要从头开始培训。几个时期的继续训练也可以利用网络态射所带来的额外容量。这些方法的一个优点是，它们允许搜索空间而不必对架构的大小有一个固有的上限；另一方面，严格的网络形态只能使架构更大，从而可能导致过于复杂的架构。这可以通过使用允许收缩体系结构的近似网络态射来减弱。</p>
<p>One-Shot Architecture Search（见图4）将所有架构视为超图（One-Shot）模型的不同子图，并在具有相同超图边缘的架构之间共享权重。只需要对一个One-Shot 模型的权重进行训练，然后可以通过从One-Shot 模型中继承训练后的权重来评估体系结构，而无需进行任何单独的训练。这大大加快了体系结构的性能评估，因为不需要训练，同样使方法只需要几个GPU天。One-Shot模型通常会产生很大的偏差，因为它严重低估了最佳体系结构的实际性能；然而，它允许对体系结构进行排名，如果估计的性能与实际性能密切相关，这就足够了。</p>
<p>不同的One-Shot NAS方法在训练One-Shot模型的方式上有所不同：ENAS学习RNN控制器，该控制器从搜索空间中采样架构，并基于通过增强获得的近似梯度训练一次性模型。DARTS通过将候选操作的混合放在one-shot模型的每个边上，结合搜索空间的连续松弛，优化了一次性模型的所有权重。与在DART中优化实值权重不同，SNAS优化了候选操作的分布。作者使用具体分布和重参数化来放松离散分布并使其可微，从而通过梯度下降实现优化。为了克服将整个一次性模型保存在GPU内存中的必要性，ProxylessNAS将体系结构权重“二进制化”，在每次操作中除了一条边外，还屏蔽了所有的边。然后，通过抽样一些二进制架构并使用BinaryConnect更新相应的概率，可以了解边缘被屏蔽或不被屏蔽的概率。</p>
<p><img src="/2020/09/26/paper-1/image-6.png" alt="img"></p>
<p>图4：one-shot架构搜索插图。与对节点应用单个操作（如3x3卷积）不同，one-shot模型（左）为每个节点包含多个候选操作，即上图中的3x3卷积（红arch 色边缘）、5x5卷积（蓝色边缘）和MaxPooling（绿色边缘）。一旦训练one-shot模型，它的权重将在不同的体系结构中共享，这些体系结构只是one-shot模型（右）的子图。</p>
<p>与这些方法相关的是超网络的元学习，它为新的架构生成权重，因此只需要训练超网络，而不需要训练架构本身。这里的主要区别是权重不是严格共享的，而是由共享超网络生成的。</p>
<h1 id="未来方向"><a href="#未来方向" class="headerlink" title="未来方向"></a>未来方向</h1><p>现有的研究主要集中在图像分类的NAS上。一方面，这提供了一个具有挑战性的基准，因为大量的人工工程都致力于寻找在该领域性能良好且不易被NAS超越的体系结构。另一方面，利用人工工程中的知识来定义一个合适的搜索空间相对容易。这反过来又使NAS不太可能找到性能大大优于现有体系结构的体系结构，因为所发现的体系结构不能有根本的区别。因此，文章认为通过将NAS应用于较少探索的领域，超越图像分类问题是很重要的。值得注意的是，在图像恢复、语义分割、转移学习、机器翻译、强化学习，以及优化递归神经网络，例如语言或音乐建模。NAS更具前景的应用领域将是生成性对抗性网络或传感器融合。</p>
<p>另一个有希望的方向是开发针对多任务问题和多目标问题的NAS方法，在这些方法中，将资源效率的度量作为目标，并将其用于未知数据的预测性能。多目标NAS与网络压缩密切相关：两者都旨在找到性能良好但高效的架构。因此，一些压缩方法也可以看作是NAS方法，反之亦然。</p>
]]></content>
      <categories>
        <category>论文学习</category>
      </categories>
      <tags>
        <tag>AutoML</tag>
        <tag>AutoDL</tag>
      </tags>
  </entry>
  <entry>
    <title>UNET3+:一种用于医学图像分割的全尺寸连接UNET</title>
    <url>/2020/12/01/paper-2/</url>
    <content><![CDATA[<p><img src="/2020/12/01/paper-2/image-1.png" alt="image-20220315200631696"></p>
<p>大家好，今天给大家分享的论文是UNET3+:一种用于医学图像分割的全尺寸连接UNET。U-NET最早出自于2015年的医学图像方面的顶会MICCAI，基本上所有的分割问题，都会先用UNET试一下结果，然后再进行各种魔改。在短短几年时间，U-NET论文的引用已达几千次，本次分享的UNET3+也是基于U-NET进行改进的，并且取得了较好的结果。</p>
<span id="more"></span>

<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>医学图像中器官的自动分割是许多临床应用的关键步骤。近年来，卷积神经网络(convolutional neural networks, CNNs)得到了极大的推动，发展出了多种分割模型，如全卷积神经网络(tional neural networks, FCNs)、UNet、PSPNet和一系列DeepLab版本。特别是基于编码-解码结构的UNet在医学图像分割中得到了广泛的应用。它使用跳跃连接来结合来自解码器的高级语义特征图和来自编码器的相应尺度的低级语义特征图。为了避免UNet中的纯跳跃连接在语义上的不相似特征的融合，UNet++通过引入嵌套的和密集的跳跃连接进一步加强了这些连接，目的是减少编码器和解码器之间的语义差距。尽管取得了良好的性能，但这种方法仍然不能从多尺度中探索足够的信息。</p>
<p>在许多分割研究中，不同尺度的特征图展示着不同的信息。低层次特征图可以捕捉丰富的空间信息，能够突出器官的边界;而高级语义特征图则体现了器官所在的位置信息。然而，当逐步下采样和上采样时，这些信息可能会逐渐稀释。为了充分利用多尺度特征，本文提出了一种新的基于U形的体系结构，命名为UNet 3+。在该网络结构中，重新设计了编码器和解码器之间的相互连接以及解码器之间的内部连接，以从全尺度捕获细粒度的细节和粗粒度的语义。为了进一步从全尺寸的聚合特征图中学习层次表示法，每个边的输出都与一个混合损失函数相连接，这有助于精确分割，特别是对于在医学图像中出现不同尺度的器官。除了提高精度外，本文还证明了所提出的UNet 3+可以减少网络参数，提高计算效率。</p>
<p>为了满足医学图像分割的准确性要求，本文进一步研究了如何有效地减少非器官图像的误报。现有的方法通过引入注意力机制或在推理时执行预定义的细化方法来解决这个问题。不同于这些方法，本文提出了一个分类任务来预测输入图像是否有器官，为分割任务提供了指导。</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p><img src="/2020/12/01/paper-2/image-2.png" alt="image-20220315200706330"> </p>
<p>图1 UNet、UNet++和UNet 3+概述。与UNet和UNet++相比，UNet 3+结合了多尺度特征，重新设计了跳跃连接，并利用多尺度的深度监督，UNet 3+使用更少的参数，但可以产生更准确的位置感知和边界增强的分割图。</p>
<h2 id="Full-scale-Skip-Connections"><a href="#Full-scale-Skip-Connections" class="headerlink" title="Full-scale Skip Connections"></a>Full-scale Skip Connections</h2><p>全尺寸跳跃连接改变了编码器和解码器之间的互连以及解码器子网之间的内连接。无论是连接简单的UNet，还是连接紧密嵌套的UNet++，都缺乏从全尺度探索足够信息的能力。为了弥补UNet和UNet++的缺陷，UNet 3+中的每一个解码器层都融合了来自编码器中的小尺度和同尺度的特征图，以及来自解码器的大尺度的特征图，这些特征图捕获了全尺度下的细粒度语义和粗粒度语义。</p>
<p><img src="/2020/12/01/paper-2/image-3.png" alt="image-20220315200722384"></p>
<p>图2构建  解码器层的特征图</p>
<p>例如，图2说明了如何构造  特征图。与UNet类似，直接接收来自相同尺度编码器层的特征图  。但不同的是，跳跃连接不止一条。其中，上面两条跳跃连接通过不同的最大池化操作将较小尺度编码器层  和  进行池化下采样，以便传递底层的低级语义信息。之所以要池化下采样，是因为要统一特征图的分辨率。从图中可知，  要缩小分辨率4倍，  要缩小分辨率2倍。另外的下面两条跳跃连接则通过双线性插值法对解码器中的  和  进行上采样从而放大特征图的分辨率，从图中可知，  (  )要放大分辨率4倍，  要放大分辨率2倍。统一完特征图之后，还需要统一特征图的数量，减少多余的信息。作者发现采用64个3×3大小的滤波器进行卷积表现效果较好。为了实现浅层精细信息与深层语义信息的无缝融合，本文进一步实现了特征聚合机制，关于特征融合一般有如下两种方法，FCN式的逐点相加或者U-Net式的通道维度拼接融合，本文是后者。这5个尺度融合后，便产生320个相同分辨率的特征图，然后再经过320个3×3大小的滤波器进行卷积 ，最后再经过BN + ReLU得到  。</p>
<p>下面从公式上表示这种Full-scale Skip Connections，i表示沿着编码方向的第i个下采样层，N表示编码器的个数(文中有5个)，那么特征图  的计算公式如下:</p>
<p><img src="/2020/12/01/paper-2/image-4.png" alt="image-20220315200746828"></p>
<p>其中，函数C表示卷积操作，函数H表示特征聚合机制(一个卷积层+一个BN+一个ReLU)，函数D和函数U分别表示上采样和下采样操作，[·]表示通道维度拼接融合。</p>
<h2 id="Full-scale-Deep-Supervision"><a href="#Full-scale-Deep-Supervision" class="headerlink" title="Full-scale Deep Supervision"></a>Full-scale Deep Supervision</h2><p>为了从全尺寸聚集特征图中学习分层表示，在UNet 3+中进一步采用了全尺寸深度监督。与在UNet++中对生成的全分辨率特征图执行的深度监督相比，本文提出的UNet 3+从每个解码器阶段产生一个侧面输出，由ground truth监督。为了实现深度监督，每个解码器的最后一层被送到一个普通的3 × 3卷积层，然后是双线性上采样和sigmoid函数。</p>
<p>为了进一步增强器官的边界，本文提出了一个多尺度结构相似度指数损失函数来赋予模糊边界更高的权重。受益于此，UNet 3+将关注模糊边界，区域分布差异越大，MS-SSIM值越高。从分割结果P和ground truth mask G裁剪了两个相应的N×N大小的块，它们可以分别表示为  和  。定义p和g的MSSSIM损失函数为:</p>
<p><img src="/2020/12/01/paper-2/image-5.png" alt="image-20220315200806107"></p>
<p>其中，M表示尺度的总数量，   ,  和  分别是p、g的均值和方差，  表示它们的协方差。   定义每个尺度中这两个部分的相对重要性。两个小的常量  和  是避免被0除。在实验中，作者将尺度设置为5(和UNet、UNet++保持一致)。结合focal损失函数, MS-SSIM损失函数和IoU损失函数,本文提出一种基于像素级、块级、图像级的分割算法，该算法能够同时捕获大尺度和精细结构，并具有清晰的边界。混合分割损失被定义为:</p>
<p><img src="/2020/12/01/paper-2/image-6.png" alt="image-20220315200820786"></p>
<h2 id="Classification-guided-Module-CGM"><a href="#Classification-guided-Module-CGM" class="headerlink" title="Classification-guided Module (CGM)"></a>Classification-guided Module (CGM)</h2><p>在大多数医学图像分割中，非器官图像出现假阳性是不可避免的。这在很大程度上是由于背景的噪声信息残留在较浅的层中，导致了过度分割的现象。为了实现更精确的分割，本文尝试通过增加一个额外的分类任务来解决这个问题，这个任务是用来预测输入图像是否有器官。</p>
<p><img src="/2020/12/01/paper-2/image-7.png" alt="image-20220315200836132"></p>
<p>图3分类引导模块(CGM)说明</p>
<p>如图3, 在经过dropout、convolution、maxpooling、sigmoid等一系列操作后，从最深处的  得到一个二维张量，每一个都表示有&#x2F;没有器官的概率。受益于丰富的语义信息，分类结果可以分两步进一步指导每个分割侧边输出。首先，利用argmax函数将二维张量转化为{0,1}的单一输出，表示有&#x2F;没有器官。随后，将单分类输出与边分割输出相乘。由于二值化分类任务的简单性，该模块在二值化交叉熵损失函数的优化下，轻松地获得了准确的分类结果，实现了对非器官图像过分割的指导。</p>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><h2 id="数据集和实现"><a href="#数据集和实现" class="headerlink" title="数据集和实现"></a>数据集和实现</h2><p>卷积神经网络与图卷积网络的连接：在肺动静脉分离中的应用</p>
<p>该方法在两个器官上得到验证:肝脏和脾脏。用于肝脏分割的数据集来自ISBI国际肝病学会2017挑战赛。它包含131个对比增强的3D腹部CT扫描，其中103个用于训练和28个用于测试。脾脏数据集来自医院，40个用于训练和9个用于测试。</p>
<h2 id="与UNet和UNet-的比较"><a href="#与UNet和UNet-的比较" class="headerlink" title="与UNet和UNet++的比较"></a>与UNet和UNet++的比较</h2><p>在这一节中，首先将UNet 3+与UNet和UNet+进行比较。每种方法中使用的损失函数是focal loss。</p>
<p>表格 1从Dice指标的角度比较UNet、UNet++、无深度监督的UNet 3+和UNet 3+。最佳结果以粗体突出显示。</p>
<p><img src="/2020/12/01/paper-2/image-8.png" alt="image-20220315200904169"></p>
<p>(一) 定量比较:基于Vgg-16和ResNet-101的主干，表1比较了UNet、UNet++和UNet 3+架构在肝脏和脾脏数据集上的参数数量和分割精度。可以看出，没有深度监督的UNet 3+的性能超过了UNet和UNet+，在两个数据集上执行的两个主干之间平均提高了2.7和1.6个点。考虑到肝脏和脾脏在CT切片中以不同的比例出现，UNet 3+结合全面的深度监督进一步提高了0.4个点。</p>
<p><img src="/2020/12/01/paper-2/image-9.png" alt="image-20220315200917848"></p>
<p>图4基于ResNet-101的UNet、UNet++和UNet 3+在肝脏数据集上的定性比较。紫色区域:真阳性(TP)；黄色区域:假阴性(FN)；绿色区域:假阳性。</p>
<p>(二) 定性比较:图4显示了基于ResNet-101的UNet、UNet++和UNet 3+的分割结果，对肝脏数据集进行了全面的深度监督。可以观察到，本文提出的方法不仅准确定位器官，而且产生连贯的边界。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文主要的贡献有三方面:</p>
<p>(一)设计了UNet 3+来充分利用多尺度特征，引入全尺度的skip connection，该连接结合了来自全尺度特征图的低级语义和高级语义，并且参数更少;</p>
<p>(二)进行深度监督，从全面的聚合特征图中学习，优化了混合损失函数以增强器官边界;</p>
<p>(三)提出分类指导模块，通过分类联合训练，减少非器官图像的过度分割</p>
]]></content>
      <categories>
        <category>论文学习</category>
      </categories>
      <tags>
        <tag>图像分割</tag>
      </tags>
  </entry>
  <entry>
    <title>动态区域感知卷积</title>
    <url>/2020/08/05/paper-3/</url>
    <content><![CDATA[<p><img src="/2020/08/05/paper-3/clip_image002.jpg" alt="img"></p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>文章提出了一种新的卷积，称为动态区域感知卷积（DRConv），它可以自动将多个滤波器分配给特征具有相似表示的相应空间区域。通过这种方式，DRConv在建模语义方面优于标准卷积。标准卷积可以增加提取更多视觉元素的通道数量，但会导致高昂的计算成本。DRConv可以将不断增加的通道滤波器转移到空间维度，从而显着提高了卷积的表示能力并保持了平移不变性。它的即插即用属性可以替代任何现有网络中的标准卷积。在各种模型（MobileNet系列，ShuffleNetV2等）和任务（分类，面部识别，检测和分割）上评估DRConv，其中在ImageNet分类上，基于DRConv的ShuffleNetV2-0.5X在46M乘加级别上实现了67.1%的最新性能，相对提高了6.3%。</p>
<span id="more"></span>

<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>当前的主流卷积运算都在空间域以共享滤波器进行，要想捕获更有效的信息，只有重复应用这些操作（例如，使用更多滤波器来增加通道和深度）。这种重复方式有几个局限性。首先，它在计算效率低下。其次，它会导致优化困难。</p>
<p>与主流卷积不同，局部卷积在空间维度上的每个像素处拥有单独的滤波器，不同地处理每个位置的特征，这比标准卷积更有效地提取空间特征。与标准卷积相比，虽然局部卷积不会增加计算复杂性，但它有两个致命的缺点。一是带来大量与空间大小成比例的参数。另一个问题是局部卷积会破坏平移不变性，而某些任务需要平移不变性（例如，局部卷积不适用于分类任务）。此外，局部卷积仍在不同样本之间共享滤波器，这使其对每个样本的特定特征不敏感。例如，在人脸识别和物体检测任务中存在具有不同姿态的样本。不同样本之间共享的滤波器无法有效提取自定义功能。</p>
<p>考虑到上述局限性，本文提出了一种新的卷积算法：动态区域感知卷积（DRConv）。DRConv具有强大的语义表示能力，可以完美地保持平移不变性。详细地说，文章设计了一个可学习的guided mask模块，可以根据其各自的特征为每个输入自动生成滤镜的区域共享模式。区域共享模式意味着将空间维度划分为多个区域，并且每个区域内仅共享一个滤波器。基于相应的输入特征，动态生成用于不同样本和不同区域的滤波器。因此，可以根据输入自动调整DRConv的滤波器，从而更有效地专注于其自身的重要特性。</p>
<p><img src="/2020/08/05/paper-3/clip_image004.jpg" alt="img"></p>
<p>DRConv的结构如图1所示。文章应用标准卷积从输入生成guided feature。根据guided feature，空间维度分为几个区域。可以看出，guided mask中相同颜色的像素被附着到相同的区域。在每个共享区域中，应用滤波器生成器模块来生成一个滤波器，以执行2D卷积运算。因此需要优化的参数主要集中在滤波器生成器模块中，其数量与空间大小无关。除了显着提高网络性能外，与局部卷积相比，DRConv可以极大地减少参数数量，与标准卷积相比，它几乎不会增加计算复杂性。</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="动态区域感知卷积"><a href="#动态区域感知卷积" class="headerlink" title="动态区域感知卷积"></a>动态区域感知卷积</h2><p>该方法主要分为两个步骤。</p>
<p>首先，使用可学习的guided mask将特征沿空间维度划分为几个区域。如图1所示，guided mask中相同颜色的像素将附加到相同区域。从图像语义的角度来看，语义相似的特征将分配给相同的区域。</p>
<p>其次，在每个共享区域中，使用滤波器生成器模块生成定制的滤波器以执行常规2D卷积操作。可以根据输入图像的重要特征自动调整自定义滤波器。</p>
<p>为了更好地说明该方法，文章主要介绍以下两个模块：可学习的guided mask模块和滤波器生成器模块。可学习的guided mask决定将哪个滤波器分配给哪个区域。滤波器生成器模块生成分配给不同区域的相应滤波器。</p>
<h2 id="可学习的guided-mask"><a href="#可学习的guided-mask" class="headerlink" title="可学习的guided mask"></a>可学习的guided mask</h2><p>对于具有m个共享区域的DRConv，用标准卷积来生成具有m个通道的guided feature。用F表示guided feature，用M表示guided mask。对于空间域中的每个位置（u，v），如图（a）所示，用argmax（·）输出其中最大值的索引。</p>
<p><img src="/2020/08/05/paper-3/clip_image006.jpg" alt="img"></p>
<p> 为了让guided mask可学习，必须得到用来生成guided feature的权值的梯度，但由于argmax(·)的使用导致guided feature的梯度无法计算，所以文章设计了图（a）所示前向传播和反向传播的梯度进行处理。</p>
<h2 id="动态滤波器：滤波器生成器模块"><a href="#动态滤波器：滤波器生成器模块" class="headerlink" title="动态滤波器：滤波器生成器模块"></a>动态滤波器：滤波器生成器模块</h2><p><img src="/2020/08/05/paper-3/clip_image008.jpg" alt="img"></p>
<p>整个滤波器生成模块如图（b）所示，首先采用一个average adaptive pooling得到k<em>k</em>C的特征图，然后接一个1x1的卷积，激活函数采用sigmoid，然后采用一个1x1卷积，没有激活函数，groups为m，得到m个k*k的卷积核。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>通过将其嵌入到包括ShuffleNetV2和MobileNet系列在内的现有流行神经网络中，来证明DRConv的有效性。将DRConv与ImageNet ，MS1M-V2 和COCO上现有的最新技术进行图像分类，面部识别，目标检测和分割方面的比较。</p>
<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p><img src="/2020/08/05/paper-3/clip_image010.jpg" alt="img"></p>
<p>为了证明DRConv的有效性，将DRConv与其他技术进行了比较。结果显示在表1中。可以看出，在具有可比的计算成本的情况下，DRConv-ShuffleNetV2在0.5x和1x比例下分别比ShufflNetV2分别实现6.3％和3.6％的增益。DRConv-MobileNetV2比MobileNetV2增长了3.7％，DRConv-MobileNetV1比基线MobileNetV1增长了4.9％。这些实验结果表明，基于DRConv的网络不仅对强大的基线有显着改进，而且与现有技术相比也有了很大的改进，证明了方法的有效性。</p>
<h2 id="人脸识别"><a href="#人脸识别" class="headerlink" title="人脸识别"></a>人脸识别</h2><p><img src="/2020/08/05/paper-3/clip_image012.jpg" alt="img"></p>
<p>为了验证DRConv的有效性，将DRConv与几种相关方法进行了比较。基于MobileFaceNet主干，只需用DRConv替换所有瓶颈块中的1×1标准卷积即可。如表2所示，DRConv-MobileFaceNet以4.9％的幅度超出基线，比CondConv提升了1.4％。为了进行进一步的比较，文章选择了局部卷积，该卷积可用于人脸识别，但需要大量参数。在设备内存的限制下，在最后三层应用局部卷积。 DRConv-MobileFaceNet（使用MobileFaceNet中的局部卷积）比Local-MobileFaceNet精度高1.3％，这进一步表明了文章提出的DRConv的优越性。</p>
<p>由于人脸数据局部统计信息的空间稳定性，DRConv的guided mask模块可以学习清晰的语义模式。如图3所示，在这些guided mask中出现了轮廓分明的面部成分。</p>
<p><img src="/2020/08/05/paper-3/clip_image014.jpg" alt="img"></p>
<h2 id="COCO对象检测与分割"><a href="#COCO对象检测与分割" class="headerlink" title="COCO对象检测与分割"></a>COCO对象检测与分割</h2><p>文章进一步评估了DRConv在对象检测和分割方面的有效性。在实验中，使用DetNAS-300M 和Mask R-CNN 框架以及FPN 和4conv1fc作为评估的基础。目标是评估用DRConvs替换DetNAS-300M和Mask R-CNN的FPN中的标准卷积时的效果，以便性能的任何改善都可以归因于DRConv的有效性。另外，将4个可学习区域，8个可学习区域和16个可学习区域设置应用于DRConv，以分析不同数量的区域的影响。</p>
<p><img src="/2020/08/05/paper-3/clip_image016.jpg" alt="img"></p>
<p>表3中显示了将DRConv与标准卷积进行比较的结果。从实验结果可以看出，DetNAS-300M中包含8个区域的DRConv算法可以将检测性能显着提高1.8%，Mask R-CNN中包含16个区域的DR-Conv算法在COCO标准AP度量上可以将检测性能提高1.2%，分割性能提高1.1%。DRConv利用guided mask将空间维度分为几组，以便每个滤波器都可以专注于特殊的环境。另一方面，像背景这样的噪声可以很容易地与其他感兴趣的区域分开，并且大多数滤波器可以集中在重要的区域上。对于不同数量的共享区域，结果表明，将空间维度划分为更多区域时，DRConv可以实现更好的性能。区域划分的增多使每个组的上下文更加专用，并且每个滤波器都可以更轻松地进行优化。</p>
<h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>本文提出了一种新的卷积算法–动态区域感知卷积算法(DRConv)，该算法成功地保持了平移不变性，完全可以成为现有网络中标准卷积的替代品。本文设计了一个小的可学习模块来预测guided mask来指导滤波器的分配，保证了一个区域内的相似特征可以匹配到相同的滤波器。此外，还设计了滤波器生成器模块，为每个样本生成定制的滤波器，使得不同的输入可以使用自己的专用滤波器。在几个不同任务上的综合实验表明了DRConv的有效性，它在分类、人脸识别、目标检测和分割方面远远优于最先进的和其他优秀的人工设计方法。</p>
]]></content>
      <categories>
        <category>论文学习</category>
      </categories>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>搜索MobileNetV3</title>
    <url>/2020/09/11/paper-4/</url>
    <content><![CDATA[<p><img src="/2020/09/11/paper-4/clip_image002.jpg" alt="img"></p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>本文提出了基于互补搜索技术与新的架构设计相结合的下一代移动网络。MobileNetV3通过结合硬件感知网络架构搜索（NAS）和NetAdapt算法进行优化，然后通过新的架构进行改进。本文探索自动搜索算法和网络设计如何协同工作，以利用互补的方法来提高整体技术水平。通过此过程，本文创建了两个新的MobileNet发行模型：MobileNetV3-Large和MobileNetV3-Small，它们分别针对高资源和低资源的用例。然后，将这些模型修改并应用于目标检测和语义分割的任务。对于语义分割的任务，本文提出了一种新的高效分割解码器Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP)。与MobileNetV2相比，MobileNetV3-Large在ImageNet分类上的准确度高3.2％，同时将延迟降低了20％。与延迟相当的MobileNetV2模型相比，MobileNetV3-Small的准确性高出6.6％。 MobileNetV3-Large的检测速度快25％以上，精度与COCO检测中的MobileNetV2相同。对于Cityscapes分割，MobileNetV3-Large LR- ASPP比MobileNetV2 R-ASPP快34％。</p>
<span id="more"></span>

<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>本文描述了开发MobileNetV3大小模型的方法，以便提供下一代高精度、高效率的神经网络模型，为设备上的计算机视觉提供动力。新的网络推动了最新技术的发展，并展示了如何将自动搜索与新的体系结构相结合来构建有效的模型。</p>
<p>本文的目的是开发最佳移动计算机视觉架构，以优化在移动设备上的精度-延迟权衡。为此，本文引入了（1）互补搜索技术，（2）新的非线性激活层h-swish，（3）新的高效网络设计（4）新型的语义分割解码器。</p>
<h1 id="高效的网络构建模块"><a href="#高效的网络构建模块" class="headerlink" title="高效的网络构建模块"></a>高效的网络构建模块</h1><p>MobileNetV1 引入了深度可分离卷积作为传统卷积层的有效替代。深度可分离卷积通过将空间滤波与特征生成机制分离而有效地分解了传统卷积。深度可分离卷积由两个单独的层定义：用于空间滤波的轻量级深度卷积和用于特征生成的1x1点卷积。</p>
<p><img src="/2020/09/11/paper-4/clip_image004.jpg" alt="img"></p>
<p>图3 MobileNetV2层</p>
<p>MobileNetV2引入了线性瓶颈和反向残差结构，以便通过利用问题的低阶性质来设计更有效的层结构。这种结构如图3所示，由1x1扩展卷积、深度卷积和1x1投影层定义。当且仅当输入和输出具有相同数量的通道时，它们才用残差连接连接。这种结构在输入和输出处保持了紧凑的表示，同时在内部扩展到更高维的特征空间，以提高非线性单通道变换的表现力。</p>
<p>MnasNet通过在瓶颈结构中引入基于squeeze and excitation的轻量级模块，构建在MobileNetV2结构之上。与基于ResNet的模块相比，squeeze and excitation模块集成在不同的位置。该模块被放置在扩展中的深度卷积之后，如图4所示。</p>
<p><img src="/2020/09/11/paper-4/clip_image006.jpg" alt="img"></p>
<p>图4 MobileNet V2 squeeze and excitation</p>
<p>对于MobileNetV3，本文使用这些层的组合作为构建块，以便构建最有效的模型，层也用改进的swish激活函数进行提升。squeeze and excitation以及swish都使用了sigmoid函数，但计算效率不高，因此本文将其替换为hard sigmoid。</p>
<h1 id="网络搜索"><a href="#网络搜索" class="headerlink" title="网络搜索"></a>网络搜索</h1><p>针对 MobileNetV3，本文先利用 NAS 优化每个网络块来搜索全局的网络架构，然后再利用 NetAdapt 算法来按顺序微调每一个单独的层。</p>
<h1 id="网络改进"><a href="#网络改进" class="headerlink" title="网络改进"></a>网络改进</h1><h2 id="重新设计代价较大的层"><a href="#重新设计代价较大的层" class="headerlink" title="重新设计代价较大的层"></a>重新设计代价较大的层</h2><p>通过架构搜索找到模型后，会发现某些最后一层以及一些较早的层比其他层代价更大。本文建议对体系结构进行一些修改，以减少这些慢速层的等待时间，同时保持精度。</p>
<p>第一个修改重做了网络的最后几层如何交互以更有效地提高最终功能。当前基于MobileNetV2的倒置瓶颈结构和变体的模型使用1x1卷积作为最后一层，以便扩展到更高维度的特征空间。该层对于具有丰富的预测功能至关重要。但是，这要付出额外的延迟。为了减少等待时间并保留高维特征，本文将这一层移至最终平均池化之后，以1x1的空间分辨率而不是7x7的空间分辨率计算出最终的要素集。</p>
<p>一旦减轻了该特征生成层的成本，就不再需要先前的瓶颈投影层来减少计算量。该观察结果使本文能够删除先前瓶颈层中的投影和过滤层，从而进一步降低了计算复杂性。原始的和优化的最后阶段可以在图5中看到。最后阶段将等待时间减少了7毫秒，这是运行时间的11％，并减少了3000万MAdds的操作次数，而几乎没有准确性的损失。</p>
<p><img src="/2020/09/11/paper-4/clip_image008.jpg" alt="img"></p>
<p>图5 原始最后阶段和有效最后阶段的比较</p>
<p>另一个代价比较大的层是初始滤波器。当前的3移动模型倾向于在完整的3x卷积中使用32个滤波器来构建用于边缘检测的初始滤波器组。本文在减少滤波器数量和使用不同的非线性来尝试和减少冗余方面进行了实验。本文决定在该层的性能以及其他经过测试的非线性上使用hard swish非线性。使用ReLU或swish，能够将滤波器数量减少到16个，同时保持与32个过滤器相同的精度。这样可以节省额外的2毫秒和1000万个MAdd。</p>
<h2 id="非线性激活函数"><a href="#非线性激活函数" class="headerlink" title="非线性激活函数"></a>非线性激活函数</h2><p>文章发现一个新的激活函数可以改进网络的性能：</p>
<p><img src="/2020/09/11/paper-4/clip_image010.jpg" alt="img"></p>
<p>这个激活函数虽好，但是在移动设备上计算开销比较大，这里对其进行了拟合改进，得到h-swish（hard swish）：</p>
<p><img src="/2020/09/11/paper-4/clip_image012.jpg" alt="img"></p>
<p>图6显示了sigmoid和swish非线性的soft和hard的比较。<img src="/2020/09/11/paper-4/clip_image014.jpg" alt="img"></p>
<p>图6 sigmoid和swish非线性</p>
<p>应用非线性的成本随着网络的深入而降低，因为每层激活存储器通常在每次分辨率下降时减半。顺便说一下，本文发现，大多数好处都是通过在更深的层面上使用它们来实现的。因此，在本文的结构中，本文只在模型的后半部分使用h-SWISH。精确布局请参阅表1和表2。</p>
<p><img src="/2020/09/11/paper-4/clip_image016.jpg" alt="img"></p>
<p>表1. MobileNetV3 Large规范 SE表示该块中是否存在squeeze-and-excite，NL表示使用的非线性类型，HS表示h-swish，RE表示relu，NBN表示无批次归一化，S表示步幅</p>
<p>即使有了这些优化，h-swish仍然会带来一些延迟开销。然而，正如所展示的那样，在没有优化的情况下，对准确性和延迟的净影响是积极的，并且在使用基于分段函数的优化实现时是显著的。<img src="/2020/09/11/paper-4/clip_image018.jpg" alt="img"></p>
<p>表2 MobileNetV3-Small规范</p>
<h2 id="Large-squeeze-and-excite"><a href="#Large-squeeze-and-excite" class="headerlink" title="Large squeeze-and-excite"></a>Large squeeze-and-excite</h2><p>Large squeeze-and-excite瓶颈的大小与卷积瓶颈的大小相关。取而代之的是，将它们全部替换为固定的，为扩展层通道数的1&#x2F;4。这样做在参数数目适度增加的情况下提高了精度，并且没有明显的延迟代价。</p>
<h2 id="MobileNetV3定义"><a href="#MobileNetV3定义" class="headerlink" title="MobileNetV3定义"></a>MobileNetV3定义</h2><p>MobileNetV3定义为两个模型：MobileNetV3-Large和MobileNetV3-Small。这些模型分别针对高资源和低资源的用例。这些模型是通过将可感知平台的NAS和NetAdapt用于网络搜索并结合本节中定义的网络改进而创建的。有关本文网络的完整规格，请参见表1和2。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>作为标准，本文将ImageNet 用于所有分类实验，并比较准确性与各种资源使用量度，例如等待时间和乘加（MAdds）。</p>
<p><img src="/2020/09/11/paper-4/clip_image020.jpg" alt="img"></p>
<p>图1 Pixel 1延迟与top-1 ImageNet精度之间的权衡</p>
<p>从图1可以看出，该模型优于当前的技术水平，例如MnasNet，ProxylessNas和MobileNetV2。</p>
<p>在表3中显示了不同Pixel手机上的性能。</p>
<p><img src="/2020/09/11/paper-4/clip_image022.jpg" alt="img"></p>
<p>表3 Pixel系列手机的浮点性能（P-n表示Pixel-n手机）</p>
<p>在表4中显示了量化结果。</p>
<p><img src="/2020/09/11/paper-4/clip_image024.jpg" alt="img"></p>
<p>表4 量化的性能</p>
<h2 id="检测"><a href="#检测" class="headerlink" title="检测"></a>检测</h2><p>本文使用MobileNetV3作为SSDLite中主干特征提取器的替代品，并与COCO数据集上的其他主干网进行了比较。</p>
<p><img src="/2020/09/11/paper-4/clip_image026.jpg" alt="img"></p>
<p>表6 在COCO测试仪上对不同的SSDLite目标检测结果</p>
<p>如表6所示，通过减少信道，MobileNetV3-Large比具有几乎相同的mAP的MobileNetV2快27％。具有通道减少功能的MobileNetV3-Small还比MobileNetV2和MnasNet高2.4和0.5 mAP，而速度却提高了35％。对于这两种MobileNetV3模型，信道减少技巧都可减少约15％的等待时间，且无mAP损失，这表明Imagenet分类和COCO对象检测可能选择不同的特征提取器形状。</p>
<h2 id="语义分割"><a href="#语义分割" class="headerlink" title="语义分割"></a>语义分割</h2><p>本文将MobileNetV2 和提出的MobileNetV3用作移动语义分段任务的网络骨干。使用mIOU 对Cityscapes数据集进行实验。</p>
<p><img src="/2020/09/11/paper-4/clip_image028.jpg" alt="img"></p>
<p>表7 Cityscapes 验证集的语义分割结果</p>
<p>如表7所示，本文注意到（1）将网络主干的最后一个块中的信道减少2倍，可以显着提高速度，同时保持类似的性能（第1行与第2行和第5行对比）与第6行相比，（2）提出的分割头LR-ASPP比R-ASPP稍快，同时性能得到了改善（第2行与第3行对比，第6行与第7行对比），（3）将分段头中的过滤器从256减少到128可提高速度，但性能会稍差一些（第3行与第4行，第7行与第8行），（4）使用相同设置时，MobileNetV3模型变体达到类似的性能，但比MobileNetV2的性能略快（行1与行5，行2与行6，行3与行7，行4与行8），（5）MobileNetV3-Small达到与MobileNetV2-2-0.5相似的性能，但速度更快；（6）MobileNetV3-Small比MobileNetV2-0.35更好，同时产生相似的速度。</p>
<p><img src="/2020/09/11/paper-4/clip_image030.jpg" alt="img"></p>
<p>表8 Cityscapes测试集上的语义分割结果</p>
<p>如表8所示，将MobileNetV3作为网络主干的分段模型的性能分别比ESPNetv2，CCC2和ESPNetv1高6.4％，10.6％，12.3％，而在MAdds方面则更快。在MobileNetV3的最后一个块中不使用原子卷积提取密集的特征图时，性能会稍微下降0.6％，但是速度提高到1.98B（对于半分辨率输入），是1.36、1.59和2.27倍分别比ESPNetv2，CCC2和ESPNetv1快。此外，本文以MobileNetV3-Small作为网络主干的模块的性能仍比所有这些模块高出至少2.1％。</p>
<h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>本文介绍了MobileNetV3 Larger和Small模型，展示了MobileNetV3在移动分类、检测和分割方面的最新进展，描述了利用多种网络架构搜索算法以及在网络设计方面的进步，还展示了如何适应非线性，如swish和应用 squeeze and excite，把它们作为有效的工具引入到移动模型中。另外还介绍了一种新的轻型分段解码器形式LR-ASPP。虽然仍然存在如何更好的将自动搜索技术与人类直觉相结合的问题，但很高兴介绍这些最初的积极成果，并将继续完善方法作为未来的工作。</p>
]]></content>
      <categories>
        <category>论文学习</category>
      </categories>
  </entry>
  <entry>
    <title>FCOS:一种简单而强大的无锚目标检测器</title>
    <url>/2020/11/01/paper-5/</url>
    <content><![CDATA[<p><img src="/2020/11/01/paper-5/image-1.png" alt="image-20220315203904180">摘要</p>
<p>在计算机视觉中，目标检测是最重要的任务之一，它支撑着一些实例级识别任务和许多下游应用。近年来，单阶段法因其设计简单、性能优越而受到越来越多的关注。本文提出了一种全卷积的单阶段目标检测器(FCOS)，以逐像素预测的方式解决目标检测问题，类似于语义分割等稠密预测问题。本文提出的检测器FCOS不含anchor，也不含proposal。FCOS通过去除预定义的锚框，避免了训练过程中计算IoU分数等与锚框相关的计算。更重要的是，还避免了所有与锚框相关的超参数，这些超参数通常对最终的检测性能非常敏感。利用唯一的后处理非极大值抑制(NMS)，演示了一个更简单和灵活的检测框架，可提高检测精度。</p>
<span id="more"></span>

<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>目标检测需要一种算法来预测图像中每个感兴趣实例的边界框位置和类别标签。在深度学习之前，滑动窗口方法是主要的方法，它对每一个可能的位置进行了详尽的分类，因此需要非常快的特征提取和分类评估。随着深度学习的发展，自从更快的R-CNN的发明以来，检测已经很大程度上转向了全卷积网络(FCNs)的使用。目前所有的主流Faster R-CNN、SSD、YOLOv2、v3等检测器都依赖一套预定义的锚框，长期以来人们认为锚框的使用是现代检测器成功的关键。尽管它们取得了巨大的成功，但需要注意的是基于锚的检测器也有一些缺点:</p>
<p>如Faster R-CNN和RetinaNet所示，检测性能对大小、长宽比和锚框数量非常敏感。例如，在RetinaNet中，改变这些超参数会在COCO基准上的AP中影响高达4%的性能。因此，需要在基于锚点的检测器中仔细地调优这些超参数。</p>
<p>即使经过仔细的设计，由于锚框的比例和长宽比是固定的，检测器在处理具有较大形状变化的候选目标时也会遇到困难，特别是对于较小的目标。预定义的锚框还会影响检测器的泛化能力，因为需要在具有不同目标大小或长宽比的新检测任务上对它们进行重新设计。</p>
<p>为了获得较高的召回率，需要使用基于锚的检测器在输入图像上密集地放置锚框(例如，对于短边为800的图像，特征金字塔网络(FPN)中锚框超过180K)。在训练过程中，大多数锚框被标记为负样本。过多的负样本增加了训练中正样本和负样本的不平衡。</p>
<p>锚框还涉及复杂的计算，比如使用GT边界框计算IoU分数。</p>
<p>近年来，FCNs在稠密预测任务中取得了巨大的成功，如语义分割，深度估计，关键点检测和计数。作为高级视觉任务之一，目标检测可能是唯一的全卷积逐像素预测框架的任务，这主要是由于锚框的使用。</p>
<p>很自然会问一个问题:是否可以用每像素预测的方式来解决目标检测，例如，类似于FCN的语义分割?因此，这些基本的视觉任务可以统一在(几乎)一个单一的框架中。本文在这项工作中表明，答案是肯定的。此外，本文还证明了，基于FCN的检测器比基于锚的检测器更简单，甚至可以获得更好的性能。</p>
<p><img src="/2020/11/01/paper-5/image-2.png" alt="image-20220315204024283"></p>
<p>图1如图所示，FCOS的工作原理是通过预测一个4D向量(l, t, r, b)来编码一个边界框在每个前景像素的位置(训练时由GT边界框监督)。右边的图显示，当一个位置位于多个边界框中时，该位置应该返回哪个边界框是不明确的。</p>
<p>在文献中，一些工作尝试利用逐像素预测FCNs进行目标检测，如DenseBox。具体地说，这些基于fcn的框架直接预测一个4D向量和一个类别在某个特征图层次上的每个空间位置。如图1(左)所示，4D向量描述了一个边界框的四个边到该位置的相对偏移量。这些框架与用于语义分割的FCNs类似，只是每个位置都需要返回一个四维连续向量。</p>
<p>但是，为了处理不同大小的边框，DenseBox对训练图像进行裁剪和调整，使其达到固定的比例。因此，DenseBox必须对图像金字塔进行检测，这与FCN一次性计算所有卷积的理念相违背。</p>
<p>此外，更值得注意的是，这些方法主要用于特殊领域的目标检测，如场景文本检测或人脸检测，因此文章认为这些方法在边界框高度重叠的通用目标检测中效果不佳。如图1(右)所示，高度重叠的边界框导致了一个棘手的模糊问题:重叠区域内的像素不清楚回归到哪个边界框。</p>
<p>在续集中，文章进一步研究这个问题，并说明使用FPN可以很大程度上消除这种模糊性。因此，本文的方法已经可以获得与传统的基于锚的检测器相似甚至更好的检测精度。此外，观察到，本文的方法可能会在远离目标目标中心的位置产生许多低质量的预测边界框。可见，靠近目标边框中心的位置可以做出更可靠的预测。因此，引入了一种新的“中心度”分数来描述某个位置对中心的偏差，如Eq.(3)所定义的，用于降低检测到的低质量边界框的重量，从而有助于抑制NMS中这些低质量检测。中心度得分由与边界框回归分支平行的一个分支(只有一层)进行预测，如图2所示。简单有效的中心分支在计算时间上有显著的提高。</p>
<p><img src="/2020/11/01/paper-5/image-3.png" alt="image-20220315204041772"></p>
<p>图2 FCOS的网络体系结构，其中C3，C4和C5表示骨干网的特征图，P3至P7是用于最终预测的特征。 H×W是特征图的高度和宽度。 “ &#x2F; s”（s &#x3D; 8、16，…，128）是特征图在相对于输入图像的下采样率。例如，所有数字都是使用800×1024输入来计算的。</p>
<p>这个新的检测框架有以下优点：</p>
<p>检测现在已经与许多其他fcn可溶性任务(如语义分割)统一起来，使重用这些任务中的思想变得更加容易。</p>
<p>检测成为无proposal和无anchor，大大减少了设计参数的数量。设计参数通常需要启发式调优，为了获得良好的性能，需要使用许多技巧。</p>
<p>通过去除锚框，完全避免了锚框在训练时的IOU计算和锚框与GT框的匹配等复杂的计算，从而比基于锚框的检测器更快的训练和测试。</p>
<p>没有花里胡哨，实现了在单阶段检测最好的结果。考虑到它提高了无锚检测器的准确性，本文鼓励社区重新考虑目标检测中锚框的是否有必要。</p>
<p>由于大大降低了设计复杂性，本文提出的检测器性能优于以前的检测器，如Faster R-CNN， RetinaNet， YOLOv3和SSD。更重要的是，由于其设计简单，FCOS只需稍加修改，就可以很容易地扩展到解决其他实例级识别任务，如实例分割、关键点检测、文本定位、跟踪等。</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>在本节中，首先以逐像素预测的方式重新制定目标检测。其次，展示了如何利用多层次预测来提高召回率和解决边界框重叠造成的模糊。最后，提出了 “中心”分支，它有助于抑制低质量的检测边界框，并大大提高了整体性能。</p>
<h2 id="全卷积单级目标检测器"><a href="#全卷积单级目标检测器" class="headerlink" title="全卷积单级目标检测器"></a>全卷积单级目标检测器</h2><p>设是CNN的第i层的feature map，s为到该层的总stride。对输入图像的GT边界框定义为  ，其中  。其中  和  表示边界框左上角和右下角的坐标。  是边界框中的目标所属的类。C是类的数量，对于MS-COCO数据集，是80个类。</p>
<p>对于feature map   上的每个位置(x, y)，其映射到输入图上的位置为  。本文的检测器直接将位置视为训练样本，而不是基于锚的检测器中的锚框，这与FCNs用于语义分割的方法相同。</p>
<p>具体地说，如果位置(x, y)落在任何GT框的中心区域，则认为它是正样本。以  为中心的框的中心区域定义为子框   ，其中s为当前feature maps的总步幅，r为COCO上的超参数为1.5。子框被剪切，使其不超出原始框。注意，这与本文最初的版本不同，先前本文认为位置只要位于ground-truth框中就是正的。除了用于分类的标签外，本文还有一个4D实向量  作为该位置的回归目标。这里  是该位置到包围框四面的距离，如图1(左)所示。如果一个位置落在多个边界框的中心区域，则认为该位置是一个模糊样本。文章先简单地选择面积最小的边界框作为回归目标。在下一节中，将展示通过多级预测可以显著减少模糊样本的数量，从而几乎不会影响检测性能。形式上，如果位置(x, y)与一个边界框  相关联，则该位置的训练回归目标可以表述为:</p>
<p>其中s为特征映射到  之前的总步幅，用于缩小回归目标并防止训练过程中梯度爆炸。值得注意的是，这与基于锚的检测器不同，每个位置只有一个锚框，关键的区别在于定义正样本和负样本的方式。单锚检测器仍然使用预定义的锚框作为先验，并使用锚框和GT框之间的IoUs来确定这些锚框的标签。在FCOS中，消除了对先验的需要，并且通过在GT框中包含位置来标记位置。在实验中，将证明使用单一锚只能取得较差的性能。</p>
<p><strong>网络输出。</strong>对应于训练目标，本文的网络的最后一层预测了一个80D向量p用于分类和一个4D向量t &#x3D; (l, t, r, b)编码边界框坐标。本文不再训练一个多类分类器，而是训练C个二分类器，在FPNs生成的feature map后，分别添加了两个分支，分别有四个卷积层(不包括最终的预测层)，分别用于分类和回归任务。此外，由于回归目标总是正的，本文使用ReLU(x)将任何实数映射到回归分支顶部的(0，∞)。值得注意的是，FCOS的网络输出变量比目前流行的基于锚点的检测器少9倍。 </p>
<p><strong>损失函数。</strong>本文对训练损失函数的定义如下:</p>
<p><img src="/2020/11/01/paper-5/image-4.png" alt="image-20220315204111876"></p>
<p>其中  是focal loss，  为GIoU loss。实验表明，GIoU损失比FCOS的初步版本中使用的IoU损失有更好的性能。本文中，  表示正样本数，  为  的平衡权值.  是指示函数。</p>
<p>推断。FCOS的推断很简单。给定一幅输入图像，通过网络传输，得到特征图Fi上每个位置的分类分数Px,y和回归预测Tx,y。在focal loss之后，选择Px,y&gt;0.05为正样本的位置，解算Eq.(1)得到预测的边界框。</p>
<h2 id="多层FPN预测FCOS"><a href="#多层FPN预测FCOS" class="headerlink" title="多层FPN预测FCOS"></a>多层FPN预测FCOS</h2><p>首先，CNN中最终feature map的大stride(例如16倍)会导致较低的best possible recall (BPR)1。对于基于锚点的检测器，通过降低正锚框的IOU分数要求，可以在一定程度上补偿由于大跨度导致的召回率低。对于FCOS，乍一看，人们可能认为BPR可以比基于锚的检测器低得多，因为它不可能在一个大stride的时候，找出一个位置没有在特征图编码的目标。这里,经验表明,即使是一个大stride,FCOS还能够产生良好的BPR。此外，通过多层FPN预测，可以进一步改进BPR。</p>
<p>其次，如图1(右)所示，groundtruth box中的重叠会导致歧义，即重叠回归中应该选择哪个边界框?这种模糊性导致性能下降。在这项工作中，本文证明了多层预测可以极大地解决模糊问题，并且FCOS可以获得与基于锚点的方法相同有时甚至更好的性能。</p>
<p>具体来说，在FPN之后，本文在不同的feature map级别上检测不同大小的目标。使用了定义为{P3, P4, P5, P6, P7}的五个层次的特征图。如图2所示，P3、P4、P5是由主干CNN的特征图C3、C4、C5产生的。在P5和P6上分别应用一个步长为2的3×3的卷积层产生P6和P7。注意，这与最初的RetinaNet不同，后者从主干特征图C5中获取P6和P7。本文发现两种方案的性能相似，但本文使用的一种参数较少。此外，P3、P4、P5、P6和P7的stride分别是8、16、32、64和128。</p>
<p>基于锚的检测器将不同尺度的锚框分配到不同的特征级别。由于锚框和真实框是通过它们的IoU分数相关联的，这使得不同的FPN特征级别能够处理不同比例的目标。但是，这将锚框的大小与每个FPN级别的目标目标的大小结合在一起，这是有问题的。锚框的大小应该是特定于数据的，可能会从一个数据集更改到另一个数据集。每个FPN级别的目标目标大小取决于FPN级别的感受野，感受野取决于网络架构。FCOS消除了耦合，因为只需要关注每个FPN级别的目标目标大小，而不需要设计锚框大小。与基于锚点的检测器不同，在FCOS中，本文直接限制了每一层的边界框回归的范围。更具体地说，本文首先计算回归目标  对于所有特征层的每个位置。接下来，如果在特征层i上的某个位置满足  或  ，那么它就被设为负样本，因此不再需要返回一个边界框。这里  是特征级别i需要返回的最大距离。本文将m2、m3、m4、m5、m6、m7分别设为0、64、128、256、512和∞。文章认为，限定最大距离是确定各特征层目标目标范围的较好方法，因为这样可以确保完整目标始终处于各特征层的接受域内。此外，由于不同大小的目标被分配到不同的特征级别，且重叠多发生在大小差别较大的目标之间，因此可以在很大程度上缓解上述模糊性。如果一个位置，即使使用多层预测，仍然分配给多个真实框，只需选择面积最小的真实框作为目标。实验结果表明，在多级预测的情况下，无锚检测器和基于锚的检测器都能达到相同的性能水平。</p>
<p>最后，在不同特征层之间共享头，不仅提高了检测器的参数效率，而且提高了检测器的检测性能。但是，文章观察到不同的特征级别需要返回不同的尺寸范围(例如P3的尺寸范围为[0,64]，P4的尺寸范围为[64,128])，因此对于不同的特征级别使用相同的头部可能不是最佳设计。在FCOS初始版本中，通过将一个可学习的标量乘以卷积层的输出来解决这个问题。在这个版本中，由于回归目标是按FPN特征级别的stride来缩小的，如Eq.(1)所示，因此标量的重要性降低了。但是，为了兼容性，文章仍然保留它们</p>
<h2 id="Center-ness-for-FCOS"><a href="#Center-ness-for-FCOS" class="headerlink" title="Center-ness for FCOS"></a>Center-ness for FCOS</h2><p><img src="/2020/11/01/paper-5/image-5.png" alt="image-20220315204144708"></p>
<p>图4 中心度。红色，蓝色和其他颜色分别表示它们之间的1，0和中间值。中心度使用例如Eq.(3)计算，并且随着位置偏离目标中心而从1衰减到0。在测试期间，将网络预测的中心度与NMS的分类分数相乘，从而能够降低由远离目标中心的位置预测的低质量边界框的权重</p>
<p>经过多级预测，FCOS已经可以取得比基于锚的RetinaNet更好的性能。此外，本文观察到有由于远离物体中心而产生的大量低质量的检测结果。本文提出了一个简单而有效的策略来抑制这些低质量的检测。具体来说，添加了一个单层分支，与回归分支并行(如图2所示)，以预测位置的“中心性”。中心度描述了该位置到该位置所负责目标中心的归一化距离，如图4所示。给定回归目标  对于一个位置，中心目标被定义为，</p>
<p><img src="/2020/11/01/paper-5/image-6.png" alt="image-20220315204200972"></p>
<p>这里用平方根来减缓中心度的衰减。中心度在0到1之间，因此使用二元交叉熵(BCE)损失进行训练。将损失加到损失函数式(2)中，检验时，最终得分s Sx,y(用于对NMS中检测进行排序)为预测的中心度Ox,y与对应的分类分数Px,y的乘积的平方根。在形式上，</p>
<p><img src="/2020/11/01/paper-5/image-7.png" alt="image-20220315204213917"></p>
<p>其中，sqrt用于校准最终分数的数量级，对平均精度(AP)没有影响。</p>
<p>因此，中心度可以降低远离物体中心的边界框的分数。因此，在最终的非最大抑制(non-maximum suppression, NMS)过程中，这些低质量的边界框很有可能被滤掉，显著提高了检测性能。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>本文的实验是在COCO上进行的。按照惯例使用COCO train2017 split (115K images)进行训练，val2017 split (5K images)作为消融研究的验证。</p>
<p><strong>训练细节</strong>。本文使用ResNet-50作为主干网，使用与RetinaNet相同的超参数。具体来说，本文的网络使用随机梯度下降(SGD)训练90k迭代，初始学习率为0.01，并有16张小批图像。在迭代60k和80k时，学习率分别降低了10倍。设定权重衰减为0.0001，动量为0.9。本文用在ImageNet上预先训练的权重初始化主干网络。除非指定，否则输入图像将被调整大小，使其短边为800，长边小于或等于1333。</p>
<p><strong>推断细节</strong>。首先通过网络将输入图像前向传播，得到预测的分类分数的预测边界框。FCOS的下一个后处理紧跟RetinaNet的后处理。后期处理超参数也是一样的，只是本文在RetinaNet中使用NMS阈值0.6而不是0.5。实验将显示NMS阈值的效果。此外，本文使用与训练时相同大小的输入图像。</p>
<h2 id="FCOS分析"><a href="#FCOS分析" class="headerlink" title="FCOS分析"></a>FCOS分析</h2><h3 id="FCOS的最佳可能召回-BPR"><a href="#FCOS的最佳可能召回-BPR" class="headerlink" title="FCOS的最佳可能召回(BPR)"></a>FCOS的最佳可能召回(BPR)</h3><p><img src="/2020/11/01/paper-5/image-8.png" alt="image-20220315204227503"></p>
<p>表1 在各种匹配规则下以及基于cocos val2017 split的FCOS的BPR时，基于锚的RetinaNet的最佳可能召回（BPR）</p>
<p>本文首先要解决的问题是FCOS可能不能提供一个良好的最佳可能召回(BPR)(即召回率的上限)。在本节中，本文通过比较FCOS的BPR和基于锚的BPR来说明这种关注是不必要的。下面的分析是基于FCOS在AdelaiDet中的实现。在形式上，BPR被定义为检测器最多能召回的真值框的数量与所有真值框的数量之比。如果将真值框分配给至少一个训练样本(即FCOS中的一个位置或基于锚点的检测器中的一个锚框)，则认为真值框被召回，并且训练采样最多可以与一个真值框相关联。如表1所示，FPN、FCOS和RetinaNet均得到相似的BPR (98.95 vs 99.32)。由于目前检测器的最佳召回率远低于90%，FCOS与基于锚定的RetinaNet之间的小BPR差距(小于0.5%)实际上不会影响检测器的性能。表3也证实了这一点，在相同的训练和测试设置下，FCOS比RetinaNet获得更好或相似的AR。更令人惊讶的是，只有当feature level P4 (stride为16)时(即没有FPN)， FCOS才能获得96.34%的不错BPR。BPR远高于官方检测中91.94%的RetinaNet的BPR，其中仅使用IOU≥0.4的低质量匹配。因此，对低BPR的担忧可能没有必要</p>
<h3 id="FCOS中的模糊样本"><a href="#FCOS中的模糊样本" class="headerlink" title="FCOS中的模糊样本"></a>FCOS中的模糊样本</h3><p>表2</p>
<p><img src="/2020/11/01/paper-5/image-9.png" alt="image-20220315204245345"> </p>
<p>基于FCN的检测器的另一个问题是，由于真值框的重叠，可能会有大量的模糊样本，如图1(右)所示。在表2中，本文给出了val2017 split上模糊样本与所有正样本的比率。如果一个位置应该与多个真实框相关联，而不使用选择面积最小的框的规则，则该位置被定义为“模糊样本”。从表中可以看出，如果不使用FPN(即只使用P4)，确实存在大量的模糊样本(23.40%)。而使用FPN，由于大部分重叠的目标被分配到不同的特征级别，所以该比例可以显著降低到7.42%。此外，如果采用中心采样，可以显著减少模糊样本。如表2所示，即使没有FPN、比值仅为3.48%。通过进一步应用FPN，该比例降至2.66%。注意，这并不是说FCOS有2.66%的位置会出错。如前所述，这些位置与与同一位置相关联的真值框中最小的一个相关联。因此，这些位置只承担丢失一些较大物体的风险。换句话说，它可能会损害FCOS的召回。但是，如表1所示，FCOS和RetinaNet之间的recall gap可以忽略不计，说明缺失物体的比例非常低。</p>
<h3 id="中心效应"><a href="#中心效应" class="headerlink" title="中心效应"></a>中心效应</h3><p>表4</p>
<p><img src="/2020/11/01/paper-5/image-10.png" alt="image-20220315204258989"></p>
<p>如前所述，本文提出“中心度”来抑制远离物体中心的位置产生的低质量检测边界框。如表4所示，中心分支可以将AP从38.0%提高到38.9%。与本文的初版本相比，这个差距相对较小，因为我们默认使用了中心采样，而且它已经消除了大量的误报。然而，这种改进仍然令人印象深刻，因为中心分支只增加了微不足道的计算时间。此外，稍后将说明，在拥挤的情况下，中心性可以带来很大的改善。有人可能会注意到，中心也可以用预测的回归向量来计算，而不引入额外的中心分支。但是，如表4所示，由回归向量计算出的中心度并不能提高性能，因此需要单独的中心度。</p>
<p>本文在图4中可视化应用中心度的效果。从图中可以看出，将中心度分值应用于分类分值后，IoU分值较低但置信分值较高的框(即图4中y &#x3D; x线下的点)被大量剔除，存在潜在的误报。</p>
<h3 id="其他的设计选择"><a href="#其他的设计选择" class="headerlink" title="其他的设计选择"></a>其他的设计选择</h3><p>表5</p>
<p><img src="/2020/11/01/paper-5/image-11.png" alt="image-20220315204327967"></p>
<p>其他的设计选择也被研究。如表5所示，在分类头和回归头中去除组标准化(GN)都会使性能下降1%。用IoU损失代替GIoU，性能下降0.3%。使用C5而不是P5也会降低性能。此外，使用P5可以减少网络参数的数量。本文还对正样本区域的半径r进行了实验。由表6可知，r &#x3D; 1.5对COCO val split的性能最好。本文还尝试将采样区域从正方形子框改为与GT框具有相同长宽比的矩形子框，从而获得相似的性能。这表明，采样区域的形状可能对最终性能不敏感。</p>
<p>表6</p>
<p><img src="/2020/11/01/paper-5/image-12.png" alt="image-20220315204340621"></p>
<p>本文还对FPN级别分配目标的不同策略进行了实验。首先，本文实验了FPN将目标proposal(即ROIs)分配给FPN级别时的分配策略。根据公式  分配目标，其中k∈{3,4,5,6,7}为目标FPN层，w和h分别为地真框的宽度和高度，  为尺度为224的目标要映射到的目标层。本文用  &#x3D; 5。如表7所示，该策略导致性能下降(37.7% AP)。文章推测这可能是由于该策略不能确保完整的目标处于目标FPN水平的接受域内。</p>
<p>表7</p>
<p><img src="/2020/11/01/paper-5/image-13.png" alt="image-20220315204359153"></p>
<p>同样，  和  也会影响性能。最终，  实现了最好的性能，因为该策略确保了完整的目标目标始终处于有效状态FPN水平的感受野。这意味着每个FPN级别(即mi)的范围超参数主要与网络架构(决定接受域)有关。</p>
<h2 id="FCOS与基于锚的检测器对比"><a href="#FCOS与基于锚的检测器对比" class="headerlink" title="FCOS与基于锚的检测器对比"></a>FCOS与基于锚的检测器对比</h2><p>本文将FCOS与基于锚的RetinaNet在基准COCO上进行了比较，证明了更简单的无锚FCOS的优越性。</p>
<p>表3 FCOS vs. RetinaNet</p>
<p><img src="/2020/11/01/paper-5/image-14.png" alt="image-20220315204414003"></p>
<h2 id="与COCO上最先进的检测器相比"><a href="#与COCO上最先进的检测器相比" class="headerlink" title="与COCO上最先进的检测器相比"></a>与COCO上最先进的检测器相比</h2><p>本文将FCOS与其他最先进的目标检测器进行了比较，结果如表8所示。</p>
<p>表8 FCOS与其他最先进的两阶段或单阶段探测器比较。FCOS的性能远远超过了最近的一些基于锚的和无锚的探测器。</p>
<p><img src="/2020/11/01/paper-5/image-15.png" alt="image-20220315204428790"></p>
<h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>本文提出了一种无anchor和无proposal的单阶段检测器FCOS。实验表明，FCOS比基于锚的单阶段检测器(包括RetinaNet、YOLO和SSD)更优且设计复杂度低得多。FCOS避免了与锚框相关的计算和超参数，以逐像素预测的方式解决了目标检测问题。FCOS在单阶段检测器中也达到了最先进的性能。</p>
]]></content>
      <categories>
        <category>论文学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>常用注解浅析</title>
    <url>/2022/03/03/community-1/</url>
    <content><![CDATA[<h1 id="1-Component-x2F-Bean-x2F-Autowired"><a href="#1-Component-x2F-Bean-x2F-Autowired" class="headerlink" title="1. @Component&#x2F;@Bean&#x2F;@Autowired"></a>1. @Component&#x2F;@Bean&#x2F;@Autowired</h1><p><code>@Component</code>（<code>@Controller</code>、<code>@Service</code>、<code>@Repository</code>）:自动创建一个实例并装配到Spring容器中(放到IOC中)</p>
<p><code>@Bean</code> :手动创建一个实例，并保留在IOC中。<br><code>@Bean</code>的好处：麻烦一点，但自定义性更强。当引用第三方库中的类需要装配到Spring容器时，则只能通过<code>@Bean</code>来实现（因为你并不能改他的源代码在他类上加个<code>@Componen</code>。<br><code>@Bean</code>注解在返回实例的方法上。</p>
<p><code>@Autowired</code>: 织入（Spring上下文已有实例（已注入IOC），<code>@Autowired</code>只是取一下）一般注解在属性上。</p>
<span id="more"></span>

<h1 id="2-RequestMapping-x2F-GetMapping-x2F-PostMapping"><a href="#2-RequestMapping-x2F-GetMapping-x2F-PostMapping" class="headerlink" title="2. @RequestMapping  &#x2F;  @GetMapping &#x2F;@PostMapping"></a>2. @RequestMapping  &#x2F;  @GetMapping &#x2F;@PostMapping</h1><p><code>@GetMapping</code>是一个组合注解，是<code>@RequestMapping</code>(method &#x3D; RequestMethod.GET)的缩写。</p>
<p>@PostMapping是一个组合注解，是<code>@RequestMapping</code>(method &#x3D; RequestMethod.POST)的缩写。</p>
<h1 id="3-RequestMapping-注解的path和value有什么区别"><a href="#3-RequestMapping-注解的path和value有什么区别" class="headerlink" title="3. @RequestMapping 注解的path和value有什么区别?"></a>3. @RequestMapping 注解的path和value有什么区别?</h1><p>从源码可以看出value和path是相互引用的，所以说——没区别。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//从源码可以看出value和path是相互引用的，所以说没区别。</span></span><br><span class="line"><span class="keyword">public</span> <span class="meta">@interface</span> RequestMapping &#123;</span><br><span class="line">    String <span class="title function_">name</span><span class="params">()</span> <span class="keyword">default</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@AliasFor(&quot;path&quot;)</span></span><br><span class="line">    String[] value() <span class="keyword">default</span> &#123;&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@AliasFor(&quot;value&quot;)</span></span><br><span class="line">    String[] path() <span class="keyword">default</span> &#123;&#125;;</span><br><span class="line"></span><br><span class="line">    RequestMethod[] method() <span class="keyword">default</span> &#123;&#125;;</span><br><span class="line"></span><br><span class="line">    String[] params() <span class="keyword">default</span> &#123;&#125;;</span><br><span class="line"></span><br><span class="line">    String[] headers() <span class="keyword">default</span> &#123;&#125;;</span><br><span class="line"></span><br><span class="line">    String[] consumes() <span class="keyword">default</span> &#123;&#125;;</span><br><span class="line"></span><br><span class="line">    String[] produces() <span class="keyword">default</span> &#123;&#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h1 id="4-Mapper和-Repository的区别"><a href="#4-Mapper和-Repository的区别" class="headerlink" title="4. @Mapper和@Repository的区别"></a>4. <code>@Mapper</code>和<code>@Repository</code>的区别</h1><h2 id="4-1-相同点"><a href="#4-1-相同点" class="headerlink" title="4.1 相同点"></a>4.1 相同点</h2><p><code>@Mapper</code>和<code>@Repository</code>都是作用在dao层接口，使得其生成代理对象bean，交给spring容器管理</p>
<h2 id="4-2-不同点"><a href="#4-2-不同点" class="headerlink" title="4.2 不同点"></a>4.2 不同点</h2><p>使用<code>@mapper</code>后，不需要在spring配置中设置扫描地址，通过mapper.xml里面的namespace属性对应相关的mapper类，spring将动态的生成Bean后注入到ServiceImpl中。</p>
<p><code>@repository</code>不可以单独使用,需要在Spring中配置扫描包地址，然后生成dao层的bean，之后被注入到ServiceImpl中</p>
]]></content>
      <categories>
        <category>Spring全家桶</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>注解</tag>
      </tags>
  </entry>
</search>
